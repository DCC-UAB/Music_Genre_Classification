{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [FMA: A Dataset For Music Analysis](https://github.com/mdeff/fma)\n",
    "\n",
    "MichaÃ«l Defferrard, Kirell Benzi, Pierre Vandergheynst, Xavier Bresson, EPFL LTS2.\n",
    "\n",
    "## Usage\n",
    "\n",
    "1. Go through the [paper] to understand what the data is about.\n",
    "1. Download some datasets from <https://github.com/mdeff/fma>.\n",
    "1. Uncompress the archives, e.g. with `unzip fma_small.zip`.\n",
    "1. Load and play with the data in this notebook.\n",
    "\n",
    "[paper]: https://arxiv.org/abs/1612.01840"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn as skl\n",
    "import sklearn.utils, sklearn.preprocessing, sklearn.decomposition, sklearn.svm\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "from utils import utils\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (17, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((106574, 52), (163, 4), (106574, 518), (13129, 249))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Directory where mp3 are stored.\n",
    "AUDIO_DIR = './data/audio/fma_small'#os.environ.get('AUDIO_DIR')\n",
    "\n",
    "# Load metadata and features.\n",
    "tracks = utils.load('./dades/fma_metadata/tracks.csv')\n",
    "genres = utils.load('./dades/fma_metadata/genres.csv')\n",
    "features = utils.load('./dades/fma_metadata/features.csv')\n",
    "echonest = utils.load('./dades/fma_metadata/echonest.csv')\n",
    "\n",
    "np.testing.assert_array_equal(features.index, tracks.index)\n",
    "assert echonest.index.isin(tracks.index).all()\n",
    "\n",
    "tracks.shape, genres.shape, features.shape, echonest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Metadata\n",
    "\n",
    "The metadata table, a CSV file in the `fma_metadata.zip` archive, is composed of many colums:\n",
    "1. The index is the ID of the song, taken from the website, used as the name of the audio file.\n",
    "2. Per-track, per-album and per-artist metadata from the Free Music Archive website.\n",
    "3. Two columns to indicate the subset (small, medium, large) and the split (training, validation, test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.display(tracks['track'].head())\n",
    "ipd.display(tracks['album'].head())\n",
    "ipd.display(tracks['artist'].head())\n",
    "ipd.display(tracks['set'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Subsets\n",
    "\n",
    "The small and medium subsets can be selected with the below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 52)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small = tracks[tracks['set', 'subset'] <= 'small']\n",
    "small.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "generes = small['track']['genres'].values\n",
    "generes_all = small['track']['genres_all'].values\n",
    "generes_top = small['track']['genre_top'].values\n",
    "\n",
    "generes_all_small = []\n",
    "generes_small = []\n",
    "generes_top_small = []\n",
    "for llista in generes:\n",
    "    for i in llista:\n",
    "        if i not in generes_small:\n",
    "            generes_small.append(i)\n",
    "for llista in generes_all:\n",
    "    for i in llista:\n",
    "        if i not in generes_all_small:\n",
    "            generes_all_small.append(i)\n",
    "for llista in generes_top:\n",
    "    for i in llista:\n",
    "        if i not in generes_top_small:\n",
    "            generes_top_small.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(generes_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(generes_all_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(generes_top_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Genres\n",
    "\n",
    "The genre hierarchy is stored in `genres.csv` and distributed in `fma_metadata.zip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{} top-level genres'.format(len(genres['top_level'].unique())))\n",
    "genres.loc[genres['top_level'].unique()].sort_values('#tracks', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres.sort_values('#tracks').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Features\n",
    "\n",
    "1. Features extracted from the audio for all tracks.\n",
    "2. For some tracks, data colected from the [Echonest](http://the.echonest.com/) API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{1} features for {0} tracks'.format(*features.shape))\n",
    "columns = ['mfcc', 'chroma_cens', 'tonnetz', 'spectral_contrast']\n",
    "columns.append(['spectral_centroid', 'spectral_bandwidth', 'spectral_rolloff'])\n",
    "columns.append(['rmse', 'zcr'])\n",
    "for column in columns:\n",
    "    ipd.display(features[column].head().style.format('{:.2f}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Echonest features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{1} features for {0} tracks'.format(*echonest.shape))\n",
    "ipd.display(echonest['echonest', 'metadata'].head())\n",
    "ipd.display(echonest['echonest', 'audio_features'].head())\n",
    "ipd.display(echonest['echonest', 'social_features'].head())\n",
    "ipd.display(echonest['echonest', 'ranks'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.display(echonest['echonest', 'temporal_features'].head())\n",
    "x = echonest.loc[2, ('echonest', 'temporal_features')]\n",
    "plt.plot(x);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Features like MFCCs are discriminant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small = tracks['set', 'subset'] <= 'small'\n",
    "genre1 = tracks['track', 'genre_top'] == 'Instrumental'\n",
    "genre2 = tracks['track', 'genre_top'] == 'Hip-Hop'\n",
    "\n",
    "X = features.loc[small & (genre1 | genre2), 'mfcc']\n",
    "X = skl.decomposition.PCA(n_components=2).fit_transform(X)\n",
    "\n",
    "y = tracks.loc[small & (genre1 | genre2), ('track', 'genre_top')]\n",
    "y = skl.preprocessing.LabelEncoder().fit_transform(y)\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], c=y, cmap='RdBu', alpha=0.5)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Audio\n",
    "\n",
    "You can load the waveform and listen to audio in the notebook itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 8000 entries, 2 to 155066\n",
      "Data columns (total 20 columns):\n",
      " #   Column         Non-Null Count  Dtype         \n",
      "---  ------         --------------  -----         \n",
      " 0   bit_rate       8000 non-null   int64         \n",
      " 1   comments       8000 non-null   int64         \n",
      " 2   composer       180 non-null    object        \n",
      " 3   date_created   8000 non-null   datetime64[ns]\n",
      " 4   date_recorded  465 non-null    datetime64[ns]\n",
      " 5   duration       8000 non-null   int64         \n",
      " 6   favorites      8000 non-null   int64         \n",
      " 7   genre_top      8000 non-null   category      \n",
      " 8   genres         8000 non-null   object        \n",
      " 9   genres_all     8000 non-null   object        \n",
      " 10  information    159 non-null    object        \n",
      " 11  interest       8000 non-null   int64         \n",
      " 12  language_code  1005 non-null   object        \n",
      " 13  license        7995 non-null   category      \n",
      " 14  listens        8000 non-null   int64         \n",
      " 15  lyricist       26 non-null     object        \n",
      " 16  number         8000 non-null   int64         \n",
      " 17  publisher      76 non-null     object        \n",
      " 18  tags           8000 non-null   object        \n",
      " 19  title          8000 non-null   object        \n",
      "dtypes: category(2), datetime64[ns](2), int64(7), object(9)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "small['track'].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "Int64Index: 106574 entries, 2 to 155320\n",
      "Series name: ('mfcc', 'kurtosis', '01')\n",
      "Non-Null Count   Dtype  \n",
      "--------------   -----  \n",
      "106574 non-null  float64\n",
      "dtypes: float64(1)\n",
      "memory usage: 1.6 MB\n"
     ]
    }
   ],
   "source": [
    "features['mfcc', 'kurtosis', '01'].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = utils.get_audio_path(AUDIO_DIR, 2)\n",
    "print('File: {}'.format(filename))\n",
    "\n",
    "x, sr = librosa.load(filename, sr=None, mono=True)\n",
    "print('Duration: {:.2f}s, {} samples'.format(x.shape[-1] / sr, x.size))\n",
    "\n",
    "start, end = 7, 17\n",
    "ipd.Audio(data=x[start*sr:end*sr], rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And use [librosa](https://github.com/librosa/librosa) to compute spectrograms and audio features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "librosa.display.waveshow(x, sr=sr);\n",
    "plt.vlines([start, end], -1, 1)\n",
    "\n",
    "start = len(x) // 2\n",
    "plt.figure()\n",
    "plt.plot(x[start:start+2000])\n",
    "plt.ylim((-1, 1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stft = np.abs(librosa.stft(x, n_fft=2048, hop_length=512))\n",
    "mel = librosa.feature.melspectrogram(sr=sr, S=stft**2)\n",
    "log_mel = librosa.amplitude_to_db(mel)\n",
    "\n",
    "librosa.display.specshow(log_mel, sr=sr, hop_length=512, x_axis='time', y_axis='mel');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc = librosa.feature.mfcc(S=librosa.power_to_db(mel), n_mfcc=20)\n",
    "mfcc = skl.preprocessing.StandardScaler().fit_transform(mfcc)\n",
    "librosa.display.specshow(mfcc, sr=sr, x_axis='time');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Genre classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 From features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small = tracks['set', 'subset'] <= 'small'\n",
    "\n",
    "train = tracks['set', 'split'] == 'training'\n",
    "val = tracks['set', 'split'] == 'validation'\n",
    "test = tracks['set', 'split'] == 'test'\n",
    "\n",
    "y_train = tracks.loc[small & train, ('track', 'genre_top')]\n",
    "y_test = tracks.loc[small & test, ('track', 'genre_top')]\n",
    "X_train = features.loc[small & train, 'mfcc']\n",
    "X_test = features.loc[small & test, 'mfcc']\n",
    "\n",
    "print('{} training examples, {} testing examples'.format(y_train.size, y_test.size))\n",
    "print('{} features, {} classes'.format(X_train.shape[1], np.unique(y_train).size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be sure training samples are shuffled.\n",
    "X_train, y_train = skl.utils.shuffle(X_train, y_train, random_state=42)\n",
    "\n",
    "# Standardize features by removing the mean and scaling to unit variance.\n",
    "scaler = skl.preprocessing.StandardScaler(copy=False)\n",
    "scaler.fit_transform(X_train)\n",
    "scaler.transform(X_test)\n",
    "\n",
    "# Support vector classification.\n",
    "clf = skl.svm.SVC()\n",
    "clf.fit(X_train, y_train)\n",
    "score = clf.score(X_test, y_test)\n",
    "print('Accuracy: {:.2%}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 From audio"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
